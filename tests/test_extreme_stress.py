#!/usr/bin/env python3
"""
üí• TESTS DE STRESS EXTR√äME - Syst√®me M√©moire Neuromorphique Jarvis
Tests destructifs pour trouver TOUS les bugs cach√©s et limites syst√®me
"""

import sys
import os
import asyncio
import time
import threading
import gc
import psutil
import random
import string
import traceback
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from unittest.mock import Mock, AsyncMock
import weakref

# Ajouter le chemin backend
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'backend'))

from memory.brain_memory_system import BrainMemorySystem, LimbicSystem
from memory.memory_types import MemoryFragment, EmotionalContext, MemoryType, ConsolidationLevel

class StressTestReporter:
    """Rapporteur de tests de stress avec m√©triques d√©taill√©es"""
    
    def __init__(self):
        self.results = {}
        self.errors = []
        self.performance_metrics = {}
        self.memory_snapshots = []
        self.start_time = time.time()
    
    def add_result(self, test_name, success, duration, details=None, error=None):
        self.results[test_name] = {
            'success': success,
            'duration': duration,
            'details': details or {},
            'error': str(error) if error else None,
            'timestamp': time.time() - self.start_time
        }
        if error:
            self.errors.append(f"{test_name}: {error}")
    
    def snapshot_memory(self, label):
        process = psutil.Process()
        snapshot = {
            'label': label,
            'rss_mb': process.memory_info().rss / 1024 / 1024,
            'vms_mb': process.memory_info().vms / 1024 / 1024,
            'cpu_percent': process.cpu_percent(),
            'threads': process.num_threads(),
            'timestamp': time.time() - self.start_time
        }
        self.memory_snapshots.append(snapshot)
        return snapshot
    
    def generate_report(self):
        total_tests = len(self.results)
        successful = sum(1 for r in self.results.values() if r['success'])
        
        return {
            'summary': {
                'total_tests': total_tests,
                'successful': successful,
                'failed': total_tests - successful,
                'success_rate': (successful / total_tests * 100) if total_tests > 0 else 0,
                'total_duration': time.time() - self.start_time,
                'error_count': len(self.errors)
            },
            'results': self.results,
            'errors': self.errors,
            'memory_snapshots': self.memory_snapshots,
            'performance_metrics': self.performance_metrics
        }

class ExtremeStressTester:
    """Testeur de stress extr√™me pour syst√®me neuromorphique"""
    
    def __init__(self):
        self.reporter = StressTestReporter()
        self.setup_mock_environment()
    
    def setup_mock_environment(self):
        """Setup environnement de test avec mocks avanc√©s"""
        self.mock_db = Mock()
        self.mock_db.save_memory_fragment = AsyncMock(return_value=True)
        self.mock_db.search_memories_hybrid = AsyncMock(return_value=[])
        
        # Mock avec d√©faillances al√©atoires
        async def unreliable_save(user_id, memory_fragment):
            if random.random() < 0.05:  # 5% de chance d'√©chec
                raise Exception("Database connection lost")
            await asyncio.sleep(random.uniform(0.001, 0.01))  # Latence variable
            return True
        
        self.mock_db.save_memory_fragment = unreliable_save
    
    async def test_massive_concurrent_load(self):
        """Test charge massive avec concurrence extr√™me"""
        print("üî• TEST 1: CHARGE MASSIVE CONCURRENTE")
        
        self.reporter.snapshot_memory("avant_charge_massive")
        
        try:
            # Cr√©er 1000 syst√®mes m√©moire simultan√©s
            systems = []
            for i in range(100):  # R√©duit pour √©viter l'explosion m√©moire
                system = BrainMemorySystem(self.mock_db)
                system.config = {'use_qdrant': False, 'auto_consolidation': False}
                systems.append(system)
            
            # 10000 interactions simultan√©es
            tasks = []
            for i in range(1000):
                system = random.choice(systems)
                user_id = f"stress_user_{i % 50}"
                message = f"Message stress concurrent #{i} " + "x" * random.randint(10, 1000)
                response = f"R√©ponse stress #{i}"
                
                # Simuler store_interaction
                async def stress_interaction(sys, uid, msg, resp):
                    try:
                        limbic = sys.limbic_system
                        context = await limbic.analyze_emotional_context(msg)
                        # Simuler stockage
                        await asyncio.sleep(random.uniform(0.001, 0.005))
                        return True
                    except Exception as e:
                        return False
                
                task = stress_interaction(system, user_id, message, response)
                tasks.append(task)
            
            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            duration = time.time() - start_time
            
            successful = sum(1 for r in results if r == True)
            failed = len(results) - successful
            exceptions = [r for r in results if isinstance(r, Exception)]
            
            self.reporter.snapshot_memory("apres_charge_massive")
            
            details = {
                'total_tasks': len(tasks),
                'successful': successful,
                'failed': failed,
                'exception_count': len(exceptions),
                'throughput': len(results) / duration,
                'avg_time_per_task': duration / len(results)
            }
            
            # Crit√®re de succ√®s: au moins 80% de r√©ussite avec 1000+ ops/sec
            success = (successful / len(results) >= 0.8) and (details['throughput'] >= 100)
            
            self.reporter.add_result("massive_concurrent_load", success, duration, details)
            
            print(f"   ‚úÖ {successful}/{len(results)} t√¢ches r√©ussies")
            print(f"   ‚úÖ D√©bit: {details['throughput']:.0f} ops/seconde")
            print(f"   ‚úÖ Exceptions: {len(exceptions)}")
            
        except Exception as e:
            self.reporter.add_result("massive_concurrent_load", False, 0, {}, e)
            print(f"   ‚ùå √âchec critique: {e}")
    
    async def test_memory_leak_detection(self):
        """Test d√©tection fuites m√©moire avec cycles longs"""
        print("üî• TEST 2: D√âTECTION FUITES M√âMOIRE")
        
        self.reporter.snapshot_memory("avant_fuite_memoire")
        
        try:
            initial_objects = len(gc.get_objects())
            
            # Cr√©er et d√©truire 1000 syst√®mes en cycles
            for cycle in range(10):
                systems = []
                
                # Cr√©er 100 syst√®mes
                for i in range(100):
                    system = BrainMemorySystem(self.mock_db)
                    system.config = {'use_qdrant': False}
                    
                    # Ajouter r√©f√©rences circulaires intentionnelles
                    system._test_circular_ref = system
                    systems.append(system)
                
                # Utiliser les syst√®mes
                tasks = []
                for system in systems[:10]:  # Utiliser seulement 10 pour performance
                    task = system.limbic_system.analyze_emotional_context(
                        f"Test cycle {cycle} " + "data" * 100
                    )
                    tasks.append(task)
                
                await asyncio.gather(*tasks, return_exceptions=True)
                
                # Nettoyer explicitement
                for system in systems:
                    system._test_circular_ref = None
                del systems
                gc.collect()
                
                # Snapshot p√©riodique
                if cycle % 3 == 0:
                    self.reporter.snapshot_memory(f"cycle_{cycle}")
            
            final_objects = len(gc.get_objects())
            object_growth = final_objects - initial_objects
            
            self.reporter.snapshot_memory("apres_fuite_memoire")
            
            # Analyse des snapshots
            memory_growth = (
                self.reporter.memory_snapshots[-1]['rss_mb'] - 
                self.reporter.memory_snapshots[-2]['rss_mb']
            )
            
            details = {
                'initial_objects': initial_objects,
                'final_objects': final_objects,
                'object_growth': object_growth,
                'memory_growth_mb': memory_growth,
                'cycles_completed': 10
            }
            
            # Crit√®re: croissance < 50 MB et < 1000 objets
            success = memory_growth < 50.0 and object_growth < 1000
            
            self.reporter.add_result("memory_leak_detection", success, 0, details)
            
            print(f"   ‚úÖ Croissance objets: {object_growth}")
            print(f"   ‚úÖ Croissance m√©moire: {memory_growth:.1f} MB")
            
        except Exception as e:
            self.reporter.add_result("memory_leak_detection", False, 0, {}, e)
            print(f"   ‚ùå √âchec: {e}")
    
    async def test_malformed_inputs(self):
        """Test inputs malform√©s et edge cases extr√™mes"""
        print("üî• TEST 3: INPUTS MALFORM√âS ET EDGE CASES")
        
        try:
            limbic = LimbicSystem()
            
            # Inputs malform√©s extr√™mes
            malformed_inputs = [
                "",  # Vide
                None,  # None
                "x" * 100000,  # Tr√®s long
                "ü§ñüíÄüëªüî•" * 1000,  # Emojis massifs
                "\x00\x01\x02" * 100,  # Caract√®res de contr√¥le
                "SELECT * FROM users; DROP TABLE users;",  # Injection SQL
                "<script>alert('xss')</script>" * 100,  # XSS
                "\n" * 1000,  # Newlines massifs
                "√©" * 10000,  # Unicode massif
                json.dumps({"nested": {"very": {"deep": "object"}}}) * 1000,  # JSON imbriqu√©
                "\\x41\\x42\\x43" * 1000,  # √âchappements
                random.random(),  # Type incorrect
                {"dict": "object"},  # Type incorrect
                [1, 2, 3, 4, 5],  # Type incorrect
            ]
            
            results = []
            for i, malformed_input in enumerate(malformed_inputs):
                try:
                    if isinstance(malformed_input, str) or malformed_input is None:
                        context = await limbic.analyze_emotional_context(
                            malformed_input or ""
                        )
                        result = "success"
                    else:
                        # Test avec conversion forc√©e
                        context = await limbic.analyze_emotional_context(str(malformed_input))
                        result = "success_converted"
                    
                    results.append(result)
                    
                except Exception as e:
                    results.append(f"error: {type(e).__name__}")
            
            # Analyser les r√©sultats
            successes = sum(1 for r in results if r.startswith("success"))
            errors = len(results) - successes
            
            details = {
                'total_inputs': len(malformed_inputs),
                'successful_handled': successes,
                'errors': errors,
                'results': results
            }
            
            # Le syst√®me doit g√©rer au moins 70% des cas sans crash
            success = successes / len(malformed_inputs) >= 0.7
            
            self.reporter.add_result("malformed_inputs", success, 0, details)
            
            print(f"   ‚úÖ G√©r√©s: {successes}/{len(malformed_inputs)}")
            print(f"   ‚úÖ Erreurs: {errors}")
            
        except Exception as e:
            self.reporter.add_result("malformed_inputs", False, 0, {}, e)
            print(f"   ‚ùå √âchec critique: {e}")
    
    async def test_race_conditions(self):
        """Test conditions de course et acc√®s concurrent aux ressources"""
        print("üî• TEST 4: RACE CONDITIONS ET CONCURRENCE")
        
        try:
            # Ressource partag√©e simul√©e
            shared_counter = {"value": 0}
            shared_data = {"memory_fragments": []}
            
            async def concurrent_access(worker_id, iterations):
                limbic = LimbicSystem()
                errors = []
                
                for i in range(iterations):
                    try:
                        # Acc√®s concurrent √† la ressource partag√©e
                        shared_counter["value"] += 1
                        
                        # Simulation d'une race condition
                        current_value = shared_counter["value"]
                        await asyncio.sleep(0.001)  # Fen√™tre pour race condition
                        
                        if current_value != shared_counter["value"] - 1:
                            # Race condition d√©tect√©e
                            pass
                        
                        # Analyse √©motionnelle avec donn√©es partag√©es
                        text = f"Worker {worker_id} iteration {i}"
                        context = await limbic.analyze_emotional_context(text)
                        
                        # Ajout concurrent √† liste partag√©e
                        shared_data["memory_fragments"].append({
                            'worker': worker_id,
                            'iteration': i,
                            'valence': context.valence,
                            'timestamp': time.time()
                        })
                        
                    except Exception as e:
                        errors.append(str(e))
                
                return len(errors)
            
            # Lancer 50 workers avec 20 it√©rations chacun
            tasks = []
            for worker_id in range(50):
                task = concurrent_access(worker_id, 20)
                tasks.append(task)
            
            start_time = time.time()
            error_counts = await asyncio.gather(*tasks, return_exceptions=True)
            duration = time.time() - start_time
            
            total_errors = sum(count for count in error_counts if isinstance(count, int))
            total_operations = 50 * 20
            final_counter = shared_counter["value"]
            fragments_count = len(shared_data["memory_fragments"])
            
            details = {
                'total_operations': total_operations,
                'total_errors': total_errors,
                'final_counter': final_counter,
                'fragments_stored': fragments_count,
                'expected_counter': total_operations,
                'duration': duration,
                'ops_per_second': total_operations / duration
            }
            
            # Succ√®s si moins de 5% d'erreurs et compteur coh√©rent
            success = (total_errors / total_operations < 0.05) and abs(final_counter - total_operations) < 100
            
            self.reporter.add_result("race_conditions", success, duration, details)
            
            print(f"   ‚úÖ Erreurs: {total_errors}/{total_operations}")
            print(f"   ‚úÖ Compteur final: {final_counter} (attendu: {total_operations})")
            print(f"   ‚úÖ Fragments: {fragments_count}")
            
        except Exception as e:
            self.reporter.add_result("race_conditions", False, 0, {}, e)
            print(f"   ‚ùå √âchec: {e}")
    
    async def test_extreme_data_sizes(self):
        """Test avec donn√©es extr√™mement volumineuses"""
        print("üî• TEST 5: DONN√âES VOLUMINEUSES EXTR√äMES")
        
        try:
            limbic = LimbicSystem()
            
            # G√©n√©rer des donn√©es de tailles croissantes
            data_sizes = [1000, 10000, 100000, 500000, 1000000]  # caract√®res
            results = {}
            
            for size in data_sizes:
                print(f"   Test avec {size} caract√®res...")
                
                # G√©n√©rer texte √©norme
                huge_text = "Test de stress m√©moire " * (size // 25) + "fin"
                huge_text = huge_text[:size]  # Tronquer exactement
                
                start_time = time.time()
                memory_before = psutil.Process().memory_info().rss / 1024 / 1024
                
                try:
                    # Tester analyse √©motionnelle
                    context = await limbic.analyze_emotional_context(huge_text)
                    
                    duration = time.time() - start_time
                    memory_after = psutil.Process().memory_info().rss / 1024 / 1024
                    memory_used = memory_after - memory_before
                    
                    results[size] = {
                        'success': True,
                        'duration': duration,
                        'memory_used_mb': memory_used,
                        'valence': context.valence,
                        'emotion': context.detected_emotion,
                        'chars_per_second': size / duration if duration > 0 else 0
                    }
                    
                    print(f"     ‚úÖ {duration:.2f}s, {memory_used:.1f} MB, {context.detected_emotion}")
                    
                    # Nettoyage forc√©
                    del huge_text
                    gc.collect()
                    
                except Exception as e:
                    results[size] = {
                        'success': False,
                        'error': str(e),
                        'duration': time.time() - start_time
                    }
                    print(f"     ‚ùå √âchec: {e}")
            
            # Analyser les r√©sultats
            successful_sizes = [size for size, result in results.items() if result.get('success', False)]
            max_successful_size = max(successful_sizes) if successful_sizes else 0
            
            details = {
                'tested_sizes': data_sizes,
                'successful_sizes': successful_sizes,
                'max_successful_size': max_successful_size,
                'results': results
            }
            
            # Succ√®s si peut traiter au moins 100K caract√®res
            success = max_successful_size >= 100000
            
            self.reporter.add_result("extreme_data_sizes", success, 0, details)
            
            print(f"   ‚úÖ Taille max r√©ussie: {max_successful_size:,} caract√®res")
            
        except Exception as e:
            self.reporter.add_result("extreme_data_sizes", False, 0, {}, e)
            print(f"   ‚ùå √âchec: {e}")
    
    async def test_error_injection_resilience(self):
        """Test r√©silience avec injection d'erreurs al√©atoires"""
        print("üî• TEST 6: R√âSILIENCE INJECTION D'ERREURS")
        
        try:
            # Simuler environnement d√©faillant
            original_methods = {}
            
            # Patch des m√©thodes pour injecter des erreurs
            def inject_random_errors(original_func, error_rate=0.1):
                async def wrapper(*args, **kwargs):
                    if random.random() < error_rate:
                        error_types = [
                            ConnectionError("Network down"),
                            TimeoutError("Operation timeout"),
                            MemoryError("Out of memory"),
                            ValueError("Invalid data"),
                            RuntimeError("System error")
                        ]
                        raise random.choice(error_types)
                    return await original_func(*args, **kwargs)
                return wrapper
            
            # Cr√©er syst√®me avec erreurs inject√©es
            system = BrainMemorySystem(self.mock_db)
            system.config = {'use_qdrant': False}
            
            # Patcher les m√©thodes critiques
            original_analyze = system.limbic_system.analyze_emotional_context
            system.limbic_system.analyze_emotional_context = inject_random_errors(
                original_analyze, 0.15
            )
            
            # Test avec 1000 op√©rations
            operations = 1000
            successes = 0
            failures = 0
            error_types = {}
            
            for i in range(operations):
                try:
                    text = f"Test r√©silience {i} avec donn√©es variables"
                    context = await system.limbic_system.analyze_emotional_context(text)
                    successes += 1
                    
                except Exception as e:
                    failures += 1
                    error_type = type(e).__name__
                    error_types[error_type] = error_types.get(error_type, 0) + 1
            
            # Restaurer m√©thodes originales
            system.limbic_system.analyze_emotional_context = original_analyze
            
            success_rate = successes / operations
            
            details = {
                'total_operations': operations,
                'successes': successes,
                'failures': failures,
                'success_rate': success_rate,
                'error_types': error_types
            }
            
            # Succ√®s si au moins 80% d'op√©rations r√©ussies malgr√© les erreurs
            success = success_rate >= 0.8
            
            self.reporter.add_result("error_injection_resilience", success, 0, details)
            
            print(f"   ‚úÖ Succ√®s: {successes}/{operations} ({success_rate*100:.1f}%)")
            print(f"   ‚úÖ Types d'erreurs: {error_types}")
            
        except Exception as e:
            self.reporter.add_result("error_injection_resilience", False, 0, {}, e)
            print(f"   ‚ùå √âchec: {e}")
    
    async def run_all_stress_tests(self):
        """Ex√©cute tous les tests de stress extr√™me"""
        print("üí• LANCEMENT TESTS DE STRESS EXTR√äME")
        print("=" * 80)
        
        self.reporter.snapshot_memory("debut_tests")
        
        # Tests de stress individuels
        tests = [
            self.test_massive_concurrent_load,
            self.test_memory_leak_detection,
            self.test_malformed_inputs,
            self.test_race_conditions,
            self.test_extreme_data_sizes,
            self.test_error_injection_resilience
        ]
        
        for test_func in tests:
            try:
                await test_func()
                print()
            except Exception as e:
                print(f"‚ùå √âCHEC CRITIQUE {test_func.__name__}: {e}")
                traceback.print_exc()
                print()
        
        self.reporter.snapshot_memory("fin_tests")
        
        # G√©n√©rer rapport final
        report = self.reporter.generate_report()
        
        print("üí• RAPPORT FINAL TESTS DE STRESS EXTR√äME")
        print("=" * 80)
        
        summary = report['summary']
        print(f"Tests ex√©cut√©s: {summary['total_tests']}")
        print(f"Succ√®s: {summary['successful']}")
        print(f"√âchecs: {summary['failed']}")
        print(f"Taux de r√©ussite: {summary['success_rate']:.1f}%")
        print(f"Dur√©e totale: {summary['total_duration']:.2f}s")
        print(f"Erreurs critiques: {summary['error_count']}")
        
        # Analyse m√©moire
        memory_start = report['memory_snapshots'][0]['rss_mb']
        memory_end = report['memory_snapshots'][-1]['rss_mb']
        memory_peak = max(s['rss_mb'] for s in report['memory_snapshots'])
        
        print(f"\nAnalyse m√©moire:")
        print(f"M√©moire d√©but: {memory_start:.1f} MB")
        print(f"M√©moire fin: {memory_end:.1f} MB")
        print(f"Pic m√©moire: {memory_peak:.1f} MB")
        print(f"Croissance totale: {memory_end - memory_start:.1f} MB")
        
        # Verdict final
        if summary['success_rate'] >= 80:
            print("\nüèÜ SYST√àME EXTR√äMEMENT ROBUSTE")
            print("‚úÖ R√©siste aux tests de stress les plus extr√™mes")
        elif summary['success_rate'] >= 60:
            print("\n‚ö†Ô∏è SYST√àME MOYENNEMENT ROBUSTE")  
            print("üîß Am√©liorations recommand√©es pour cas extr√™mes")
        else:
            print("\n‚ùå SYST√àME FRAGILE")
            print("üö´ Corrections critiques requises")
        
        return report

async def main():
    """Fonction principale des tests de stress extr√™me"""
    tester = ExtremeStressTester()
    report = await tester.run_all_stress_tests()
    
    # Sauvegarder rapport
    import json
    with open('/home/enzo/Documents/Projet Jarvis/tests/extreme_stress_report.json', 'w') as f:
        json.dump(report, f, indent=2, default=str)
    
    return report['summary']['success_rate'] >= 60

if __name__ == "__main__":
    import json
    result = asyncio.run(main())
    sys.exit(0 if result else 1)