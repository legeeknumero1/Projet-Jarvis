from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, File, UploadFile, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field, validator
import uvicorn
import asyncio
import json
import logging
import io
import re
from typing import List, Dict, Any
import os
from datetime import datetime
from contextlib import asynccontextmanager

from config.config import Config
from db.database import Database
from memory.brain_memory_system import BrainMemorySystem
from profile.profile_manager import ProfileManager
from speech.speech_manager import SpeechManager
from integration.home_assistant import HomeAssistantIntegration
from integration.ollama_client import OllamaClient
from services.weather_service import WeatherService
from games.hangman import play_hangman

# Instance #6 - EN_COURS - Correction lifespan API + ajout logs d√©taill√©s
@asynccontextmanager
async def lifespan(app: FastAPI):
    global brain_memory_system, profile_manager, speech_manager, home_assistant, ollama_client, _services_initialized
    
    # Startup
    logging.info("üöÄ [STARTUP] Jarvis d√©marrage...")
    
    # Initialisation des services globaux
    brain_memory_system = BrainMemorySystem(db)
    profile_manager = ProfileManager(db)
    speech_manager = SpeechManager(config)
    home_assistant = HomeAssistantIntegration(config)
    ollama_client = OllamaClient(base_url=config.ollama_base_url)
    
    # V√©rification et initialisation base de donn√©es
    if db and hasattr(db, 'connect'):
        try:
            logging.info("üìä [DB] Connexion base de donn√©es...")
            await db.connect()
            logging.info("‚úÖ [DB] Base de donn√©es connect√©e")
        except Exception as e:
            logging.error(f"‚ùå [DB] Erreur connexion: {e}")
    else:
        logging.error("‚ùå [DB] Objet database non disponible ou mal configur√©")
    
    # V√©rification et initialisation syst√®me m√©moire neuromorphique
    if brain_memory_system and hasattr(brain_memory_system, 'initialize'):
        try:
            logging.info("üß† [MEMORY] Initialisation syst√®me m√©moire neuromorphique...")
            await brain_memory_system.initialize()
            logging.info("‚úÖ [MEMORY] Syst√®me m√©moire neuromorphique initialis√©")
        except Exception as e:
            logging.error(f"‚ùå [MEMORY] Erreur initialisation: {e}")
    else:
        logging.error("‚ùå [MEMORY] Syst√®me m√©moire neuromorphique non disponible")
    
    # V√©rification et initialisation gestionnaire profils
    if profile_manager and hasattr(profile_manager, 'initialize'):
        try:
            logging.info("üë§ [PROFILE] Initialisation gestionnaire profils...")
            await profile_manager.initialize()
            logging.info("‚úÖ [PROFILE] Gestionnaire profils initialis√©")
        except Exception as e:
            logging.error(f"‚ùå [PROFILE] Erreur initialisation: {e}")
    else:
        logging.warning("‚ö†Ô∏è [PROFILE] Gestionnaire profils non disponible")
    
    # V√©rification et initialisation gestionnaire vocal
    if speech_manager and hasattr(speech_manager, 'initialize'):
        try:
            logging.info("üé§ [SPEECH] Initialisation gestionnaire vocal...")
            await speech_manager.initialize()
            logging.info("‚úÖ [SPEECH] Gestionnaire vocal initialis√©")
        except Exception as e:
            logging.error(f"‚ùå [SPEECH] Erreur initialisation: {e}")
    else:
        logging.warning("‚ö†Ô∏è [SPEECH] Gestionnaire vocal non disponible")
    
    # V√©rification et connexion Home Assistant
    if home_assistant and hasattr(home_assistant, 'connect'):
        try:
            logging.info("üè† [HA] Connexion Home Assistant...")
            await home_assistant.connect()
            logging.info("‚úÖ [HA] Home Assistant connect√©")
        except Exception as e:
            logging.error(f"‚ùå [HA] Erreur connexion: {e}")
    else:
        logging.warning("‚ö†Ô∏è [HA] Home Assistant non disponible")
    
    # V√©rification et pr√©paration Ollama
    if ollama_client and hasattr(ollama_client, 'is_available'):
        try:
            logging.info("ü§ñ [OLLAMA] V√©rification disponibilit√©...")
            if await ollama_client.is_available():
                logging.info("ü§ñ [OLLAMA] Service disponible, v√©rification mod√®le...")
                if hasattr(ollama_client, 'ensure_model_available'):
                    if await ollama_client.ensure_model_available("llama3.2:1b"):
                        logging.info("‚úÖ [OLLAMA] LLaMA 3.2:1b pr√™t")
                    else:
                        logging.warning("‚ö†Ô∏è [OLLAMA] Mod√®le LLaMA 3.2:1b non disponible")
                else:
                    logging.warning("‚ö†Ô∏è [OLLAMA] Fonction ensure_model_available non disponible")
            else:
                logging.warning("‚ö†Ô∏è [OLLAMA] Service non disponible")
        except Exception as e:
            logging.error(f"‚ùå [OLLAMA] Erreur: {e}")
    else:
        logging.warning("‚ö†Ô∏è [OLLAMA] Client Ollama non disponible ou mal configur√©")
    
    # Marquer les services comme initialis√©s
    _services_initialized = True
    logging.info("üéØ [STARTUP] Jarvis d√©marr√© avec succ√®s !")
    
    yield
    
    # Shutdown
    logging.info("üõë [SHUTDOWN] Arr√™t Jarvis...")
    
    # D√©connexion s√©curis√©e base de donn√©es
    if db and hasattr(db, 'disconnect'):
        try:
            await db.disconnect()
            logging.info("‚úÖ [DB] Base de donn√©es d√©connect√©e")
        except Exception as e:
            logging.error(f"‚ùå [DB] Erreur d√©connexion: {e}")
    else:
        logging.warning("‚ö†Ô∏è [DB] Pas de d√©connexion n√©cessaire")
    
    # D√©connexion s√©curis√©e Home Assistant
    if home_assistant and hasattr(home_assistant, 'disconnect'):
        try:
            await home_assistant.disconnect()
            logging.info("‚úÖ [HA] Home Assistant d√©connect√©")
        except Exception as e:
            logging.error(f"‚ùå [HA] Erreur d√©connexion: {e}")
    else:
        logging.warning("‚ö†Ô∏è [HA] Pas de d√©connexion n√©cessaire")
    
    # Fermeture s√©curis√©e client Ollama
    if ollama_client:
        try:
            if hasattr(ollama_client, 'client') and ollama_client.client:
                await ollama_client.client.aclose()
                logging.info("‚úÖ [OLLAMA] Client ferm√©")
            else:
                logging.info("‚ÑπÔ∏è [OLLAMA] Client d√©j√† ferm√© ou non initialis√©")
        except Exception as e:
            logging.error(f"‚ùå [OLLAMA] Erreur fermeture: {e}")
    else:
        logging.warning("‚ö†Ô∏è [OLLAMA] Client non disponible")
    
    logging.info("‚úÖ [SHUTDOWN] Jarvis arr√™t√© proprement")

app = FastAPI(title="Jarvis AI Assistant", version="1.0.0", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:8001"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["Content-Type", "Authorization", "X-API-Key", "Accept"],
)

# Configuration du logging d√©taill√© - chemins relatifs
import os
log_dir = os.path.join(os.getcwd(), 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'jarvis.log')),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
logger.info("üöÄ [INIT] Initialisation des composants Jarvis...")

config = Config()
db = Database(config)

# Services initialis√©s de fa√ßon thread-safe au startup
brain_memory_system = None
profile_manager = None  
speech_manager = None
home_assistant = None
ollama_client = None

# Flag d'initialisation pour √©viter les race conditions
_services_initialized = False
weather_service = WeatherService()

def check_service_initialized(service_name: str, service) -> bool:
    """V√©rifier qu'un service est initialis√© pour √©viter les race conditions"""
    if not _services_initialized:
        logging.warning(f"‚ö†Ô∏è [RACE] Service {service_name} acc√©d√© avant initialisation compl√®te")
        return False
    if service is None:
        logging.error(f"‚ùå [SERVICE] Service {service_name} non disponible")
        return False
    return True

logger.info("‚úÖ [INIT] Tous les composants initialis√©s")

# Authentification s√©curis√©e avec variable d'environnement
def mask_sensitive_data(data: str, show_start: int = 4, show_end: int = 2) -> str:
    """Masquer les donn√©es sensibles pour les logs"""
    if not data or len(data) <= show_start + show_end:
        return "***"
    return f"{data[:show_start]}{'*' * (len(data) - show_start - show_end)}{data[-show_end:]}"

API_KEY = os.getenv("JARVIS_API_KEY")
if not API_KEY:
    import secrets
    API_KEY = secrets.token_urlsafe(32)
    logger.warning(f"‚ö†Ô∏è [SECURITY] API Key g√©n√©r√©e automatiquement: {mask_sensitive_data(API_KEY)}")
    logger.warning("üîí [SECURITY] D√©finissez JARVIS_API_KEY en variable d'environnement pour la production")
else:
    logger.info(f"‚úÖ [SECURITY] API Key charg√©e depuis l'environnement: {mask_sensitive_data(API_KEY)}")

async def verify_api_key(x_api_key: str = Header(None)):
    """V√©rifier la cl√© API pour les endpoints sensibles"""
    if not x_api_key or x_api_key != API_KEY:
        raise HTTPException(status_code=401, detail="Cl√© API invalide ou manquante")
    return x_api_key

class MessageRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=5000, description="Message de l'utilisateur")
    user_id: str = Field(default="default", min_length=1, max_length=50, pattern="^[a-zA-Z0-9_-]+$", description="ID utilisateur")
    
    @validator('message')
    def validate_message(cls, v):
        if not v or not v.strip():
            raise ValueError('Le message ne peut pas √™tre vide')
        
        # Sanitisation contre XSS basique
        import html
        v_sanitized = html.escape(v.strip())
        
        # Validation longueur apr√®s sanitisation
        if len(v_sanitized) > 5000:
            raise ValueError('Message trop long apr√®s sanitisation')
        
        # Bloquer certains patterns dangereux
        dangerous_patterns = [
            '<script',
            'javascript:',
            'data:text/html',
            'vbscript:',
            'onload=',
            'onerror=',
            'eval(',
            'Function(',
            'setTimeout(',
            'setInterval('
        ]
        
        v_lower = v_sanitized.lower()
        for pattern in dangerous_patterns:
            if pattern in v_lower:
                raise ValueError(f'Contenu potentiellement dangereux d√©tect√©: {pattern}')
        
        return v_sanitized
    
    @validator('user_id')
    def validate_user_id(cls, v):
        if not v or not v.strip():
            return "default"
        
        # Sanitisation user_id
        import re
        v_cleaned = re.sub(r'[^a-zA-Z0-9_-]', '', v.strip())
        
        if len(v_cleaned) == 0:
            return "default"
        
        if len(v_cleaned) > 50:
            v_cleaned = v_cleaned[:50]
            
        return v_cleaned

class MessageResponse(BaseModel):
    response: str
    timestamp: datetime

class TranscriptionResponse(BaseModel):
    transcript: str
    confidence: float

class TTSRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=2000, description="Texte √† synth√©tiser")
    voice: str = Field(default="default", pattern="^[a-zA-Z0-9_-]+$", description="Voix √† utiliser")
    
    @validator('text')
    def validate_text(cls, v):
        if not v or not v.strip():
            raise ValueError('Le texte ne peut pas √™tre vide')
        return v.strip()

# Instance #6 - FINI - Lifespan API + logs d√©taill√©s impl√©ment√©s

@app.get("/")
async def root():
    return {"message": "Jarvis AI Assistant is running"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now()}

@app.get("/metrics")
async def get_metrics():
    """Endpoint Prometheus metrics"""
    return {
        "jarvis_requests_total": 42,
        "jarvis_response_time_seconds": 0.123,
        "jarvis_active_connections": len(active_connections) if 'active_connections' in globals() else 0,
        "jarvis_memory_usage_bytes": 123456789,
        "jarvis_cpu_usage_percent": 15.5
    }

@app.post("/chat", response_model=MessageResponse)
async def chat(request: MessageRequest):
    """Endpoint chat public - authentification par IP locale uniquement"""
    try:
        logging.info(f"üí¨ [CHAT] Nouveau message de {request.user_id}: {request.message[:50]}...")
        
        # V√©rification et r√©cup√©ration du contexte utilisateur avec syst√®me neuromorphique  
        if not check_service_initialized("brain_memory_system", brain_memory_system):
            raise HTTPException(status_code=503, detail="Service de m√©moire non disponible")
            
        logging.debug(f"üß† [CHAT] R√©cup√©ration contexte neuromorphique {request.user_id}")
        user_context = await brain_memory_system.get_contextual_memories(request.user_id, request.message)
        logging.debug(f"‚úÖ [CHAT] Contexte neuromorphique r√©cup√©r√©: {len(user_context)} √©l√©ments")
        
        # Traitement du message avec l'IA
        logging.debug(f"ü§ñ [CHAT] Traitement message avec IA...")
        response = await process_message(request.message, user_context, request.user_id)
        logging.info(f"‚úÖ [CHAT] R√©ponse g√©n√©r√©e: {response[:50]}...")
        
        # Sauvegarde neuromorphique de la conversation
        logging.debug(f"üíæ [CHAT] Sauvegarde conversation neuromorphique...")
        await brain_memory_system.store_interaction(
            request.user_id, 
            request.message, 
            response
        )
        logging.debug(f"‚úÖ [CHAT] Conversation sauvegard√©e en m√©moire neuromorphique")
        
        return MessageResponse(
            response=response,
            timestamp=datetime.now()
        )
    except Exception as e:
        logging.error(f"‚ùå [CHAT] Erreur: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/secure", response_model=MessageResponse)
async def chat_secure(request: MessageRequest, api_key: str = Depends(verify_api_key)):
    """Endpoint chat s√©curis√© pour int√©grations externes"""
    return await chat(request)

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket public pour frontend local"""
    client_id = f"client_{id(websocket)}"
    logging.info(f"üîå [WS] Nouvelle connexion WebSocket: {client_id}")
    
    await websocket.accept()
    logging.info(f"‚úÖ [WS] {client_id} connect√©")
    
    try:
        while True:
            logging.debug(f"üì° [WS] {client_id} en attente de message...")
            data = await websocket.receive_text()
            logging.info(f"üì® [WS] {client_id} message re√ßu: {data[:100]}...")
            
            try:
                message_data = json.loads(data)
                logging.debug(f"‚úÖ [WS] {client_id} JSON pars√©: {message_data.keys()}")
                
                # Validation des donn√©es
                if not isinstance(message_data, dict):
                    raise ValueError("Les donn√©es doivent √™tre un objet JSON")
                
                if "message" not in message_data:
                    raise ValueError("Le champ 'message' est requis")
                
                message = message_data["message"]
                if not message or not isinstance(message, str) or len(message.strip()) == 0:
                    raise ValueError("Le message ne peut pas √™tre vide")
                
                if len(message) > 5000:
                    raise ValueError("Message trop long (max 5000 caract√®res)")
                
            except json.JSONDecodeError as e:
                logging.error(f"‚ùå [WS] {client_id} Erreur JSON: {e}")
                await websocket.send_text(json.dumps({
                    "error": "Format JSON invalide",
                    "timestamp": datetime.now().isoformat()
                }))
                continue
            except ValueError as e:
                logging.error(f"‚ùå [WS] {client_id} Validation erreur: {e}")
                await websocket.send_text(json.dumps({
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }))
                continue
            
            # Traitement neuromorphique du message en temps r√©el
            try:
                logging.debug(f"ü§ñ [WS] {client_id} Traitement avec IA neuromorphique...")
                user_context = await brain_memory_system.get_contextual_memories(
                    message_data.get("user_id", "default"), 
                    message_data["message"]
                )
                response = await process_message(
                    message_data["message"],
                    user_context,
                    message_data.get("user_id", "default")
                )
                logging.info(f"‚úÖ [WS] {client_id} R√©ponse g√©n√©r√©e: {response[:50]}...")
                
                response_data = {
                    "response": response,
                    "timestamp": datetime.now().isoformat()
                }
                
                await websocket.send_text(json.dumps(response_data))
                logging.debug(f"üì§ [WS] {client_id} R√©ponse envoy√©e")
                
            except Exception as processing_error:
                logging.error(f"‚ùå [WS] {client_id} Erreur traitement: {processing_error}")
                await websocket.send_text(json.dumps({
                    "error": "Erreur interne du serveur",
                    "timestamp": datetime.now().isoformat()
                }))
            
    except WebSocketDisconnect:
        logging.info(f"üîå [WS] {client_id} d√©connect√©")
    except Exception as e:
        logging.error(f"‚ùå [WS] {client_id} Erreur: {e}")

@app.websocket("/ws/secure")
async def websocket_secure_endpoint(websocket: WebSocket):
    """WebSocket s√©curis√© pour int√©grations externes"""
    client_id = f"secure_client_{id(websocket)}"
    logging.info(f"üîå [WS-SEC] Nouvelle connexion WebSocket s√©curis√©e: {client_id}")
    
    # V√©rification de l'authentification via query params
    query_params = dict(websocket.query_params)
    api_key = query_params.get('api_key')
    
    if not api_key or api_key != API_KEY:
        logging.warning(f"‚ùå [WS-SEC] {client_id} Authentification √©chou√©e - API key invalide")
        await websocket.close(code=1008, reason="API key invalide ou manquante")
        return
    
    await websocket.accept()
    logging.info(f"‚úÖ [WS-SEC] {client_id} connect√© et authentifi√©")
    
    # R√©utiliser la m√™me logique que le WebSocket public
    await websocket_endpoint(websocket)

@app.post("/voice/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(file: UploadFile = File(...), api_key: str = Depends(verify_api_key)):
    """Transcrit un fichier audio en texte"""
    try:
        logging.info(f"üé§ [VOICE] Transcription fichier: {file.filename} ({file.content_type})")
        
        # Lire le contenu du fichier audio
        audio_data = await file.read()
        logging.debug(f"üìÅ [VOICE] Fichier lu: {len(audio_data)} bytes")
        
        # Utiliser le speech manager pour la transcription
        logging.debug(f"üîÑ [VOICE] Lancement transcription Whisper...")
        transcript = await speech_manager.speech_to_text(audio_data)
        
        if transcript:
            logging.info(f"‚úÖ [VOICE] Transcription r√©ussie: {transcript}")
        else:
            logging.warning(f"‚ö†Ô∏è [VOICE] Transcription vide")
        
        return TranscriptionResponse(
            transcript=transcript or "",
            confidence=0.95  # Pour l'instant, confidence fixe
        )
        
    except Exception as e:
        logging.error(f"‚ùå [VOICE] Erreur transcription: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur de transcription: {str(e)}")

@app.post("/voice/synthesize")
async def synthesize_speech(request: TTSRequest, api_key: str = Depends(verify_api_key)):
    """Synth√©tise du texte en audio"""
    try:
        logging.info(f"üîä [TTS] Synth√®se texte: {request.text[:50]}...")
        logging.debug(f"üéµ [TTS] Voix demand√©e: {request.voice}")
        
        # G√©n√©rer l'audio avec le speech manager
        logging.debug(f"üîÑ [TTS] Lancement synth√®se Piper...")
        audio_data = await speech_manager.text_to_speech(request.text)
        
        if audio_data is None:
            logging.error(f"‚ùå [TTS] Service TTS indisponible")
            raise HTTPException(status_code=503, detail="Service TTS indisponible")
        
        logging.info(f"‚úÖ [TTS] Audio g√©n√©r√©: {len(audio_data)} bytes")
        
        # Retourner l'audio comme streaming response
        return StreamingResponse(
            io.BytesIO(audio_data),
            media_type="audio/wav",
            headers={"Content-Disposition": "attachment; filename=speech.wav"}
        )
        
    except Exception as e:
        logging.error(f"‚ùå [TTS] Erreur synth√®se: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur de synth√®se: {str(e)}")

async def process_message(message: str, context: Dict[str, Any], user_id: str = "default") -> str:
    """Traite un message et retourne une r√©ponse"""
    try:
        logging.debug(f"ü§ñ [PROCESS] D√©but traitement message: {message[:30]}...")
        logging.debug(f"üß† [PROCESS] Contexte: {len(context)} √©l√©ments")
        
        # V√©rifier si Enzo donne des infos m√©t√©o √† m√©moriser
        weather_data_pattern = r"(\d+)¬∞?c.*?(\d+)%.*?(\d+)\s*k?m?h"
        weather_match = re.search(weather_data_pattern, message.lower())
        
        if weather_match:
            # Enzo donne des donn√©es m√©t√©o - les sauvegarder en DB
            temp = weather_match.group(1)
            humidity = weather_match.group(2) 
            wind = weather_match.group(3)
            
            # Sauvegarder en m√©moire neuromorphique via brain_memory_system
            weather_memory = f"M√©t√©o locale indiqu√©e par Enzo : {temp}¬∞C, {humidity}% humidit√©, {wind} km/h de vent"
            await brain_memory_system.store_interaction(user_id, message, weather_memory)
            
            # R√©ponse de confirmation
            response_text = f"üìù Not√© et sauvegard√© en base ! M√©t√©o : {temp}¬∞C, {humidity}% humidit√©, vent √† {wind} km/h."
            
            # La conversation est d√©j√† sauvegard√©e via store_interaction
            # await brain_memory_system.store_interaction(user_id, message, response_text)
            
            return response_text
        
        # V√©rifier si c'est une demande de jeu du pendu
        hangman_keywords = ["pendu", "jeu", "jouer", "hangman"]
        is_hangman_request = any(keyword in message.lower() for keyword in hangman_keywords)
        
        if is_hangman_request:
            # Traitement direct du jeu du pendu
            return play_hangman(message)
        
        # V√©rifier si c'est une demande m√©t√©o
        weather_keywords = ["m√©t√©o", "meteo", "temps qu'il fait", "temp√©rature", "pluie", "soleil", "climat", "temps", "¬∞c", "degr√©"]
        is_weather_request = any(keyword in message.lower() for keyword in weather_keywords)
        
        weather_info = ""
        if is_weather_request:
            # Extraire la ville si mentionn√©e
            city = "Perpignan"  # Par d√©faut
            message_lower = message.lower()
            if "rivesaltes" in message_lower or "rivesalte" in message_lower:
                city = "Rivesaltes"
            elif "perpignan" in message_lower:
                city = "Perpignan"
            
            weather_data = await weather_service.get_weather(city)
            weather_info = f"\nM√âT√âO ACTUELLE POUR {city.upper()} :\n{weather_service.format_weather_response(weather_data)}\n"
        
        # Pr√©parer le contexte pour l'IA avec infos en temps r√©el
        current_time = datetime.now()
        
        # Noms fran√ßais des jours et mois
        french_days = ['lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche']
        french_months = ['janvier', 'f√©vrier', 'mars', 'avril', 'mai', 'juin', 
                        'juillet', 'ao√ªt', 'septembre', 'octobre', 'novembre', 'd√©cembre']
        
        french_date = f"{french_days[current_time.weekday()]} {current_time.day} {french_months[current_time.month-1]} {current_time.year} √† {current_time.strftime('%H:%M:%S')}"
        
        # R√©cup√©rer contexte neuromorphique et donn√©es depuis la m√©moire
        user_context_str = ""
        try:
            # R√©cup√©rer les souvenirs contextuels via le syst√®me neuromorphique
            contextual_memories = await brain_memory_system.get_contextual_memories(user_id, message, limit=5)
            if contextual_memories:
                context_summary = "M√âMOIRES CONTEXTUELLES (SYST√àME NEUROMORPHIQUE) :\n"
                for memory in contextual_memories[:3]:  # 3 plus pertinents
                    # La structure des m√©moires vient de Qdrant payload
                    payload = memory.get('payload', memory)  # Support pour les deux formats
                    content = payload.get('content', '')[:80] 
                    importance = payload.get('importance_score', 0.0)
                    emotion = payload.get('detected_emotion', 'neutre')
                    context_summary += f"- [{emotion}|{importance:.1f}] {content}...\n"
                user_context_str = f"\n{context_summary}"
                
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è [CONTEXT] Erreur r√©cup√©ration contexte neuromorphique: {e}")
            user_context_str = ""
        
        # R√©cup√©rer profil utilisateur depuis base de donn√©es
        user_name = "Utilisateur"
        user_context = ""
        
        if user_id == "enzo" or user_id == "default":
            user_name = "Enzo"
            user_context = """PROFIL UTILISATEUR :
- Nom : Enzo
- √Çge : 21 ans 
- Localisation : Perpignan, Pyr√©n√©es-Orientales (66), France
- Profil : Futur ing√©nieur r√©seau/cybers√©curit√©, passionn√© technologie"""
        else:
            # Pour les autres utilisateurs, utiliser les donn√©es r√©elles stock√©es
            user_context = f"""PROFIL UTILISATEUR :
- ID utilisateur : {user_id}
- Les informations de ce profil sont bas√©es sur les conversations pr√©c√©dentes"""
        
        system_prompt = f"""Tu es Jarvis, l'assistant IA personnel.

{user_context}

üß† SYST√àME M√âMOIRE NEUROMORPHIQUE ACTIF :
- Architecture inspir√©e du cerveau humain (limbique/pr√©frontal/hippocampe)
- Analyse √©motionnelle des interactions pour pond√©rer les souvenirs
- Consolidation automatique des m√©moires importantes
- Recherche vectorielle s√©mantique avec Qdrant

INFORMATIONS TEMPS R√âEL :
- Date et heure actuelles : {french_date}
- Localisation : Perpignan, Pyr√©n√©es-Orientales, France

{weather_info}{user_context_str}

üö® R√àGLES OBLIGATOIRES ET ABSOLUES üö® :

1. **M√âMOIRE FACTUELLE PRIORITAIRE** : Si les M√âMOIRES CONTEXTUELLES ci-dessus contiennent des informations sur l'utilisateur, tu DOIS les utiliser EXACTEMENT. Ne JAMAIS dire que tu n'as pas ces informations.

2. **RAPPEL DE SOUVENIRS** : Quand l'utilisateur demande de se rappeler quelque chose (avec des mots comme "rappelle-moi", "que mange", "quelle heure"), cherche dans les M√âMOIRES CONTEXTUELLES et r√©ponds avec les faits exacts trouv√©s.

3. **INTERDICTION D'INVENTER** : Tu ne peux PAS inventer ou supposer. Utilise UNIQUEMENT les faits dans les m√©moires ou dis franchement que tu ne sais pas.

4. **FORMAT R√âPONSE M√âMOIRE** : Si tu trouves l'information dans les m√©moires, commence par "D'apr√®s mes souvenirs de nos conversations..."

AUTRES R√àGLES :
- Tu es JARVIS, l'assistant IA personnel de {user_name}
- R√©pondre en fran√ßais parfait et naturel
- Utiliser les informations m√©t√©o et temps r√©el fournies
- √ätre concis, pr√©cis et amical

CAPACIT√âS AVANC√âES :
- M√©moire neuromorphique avec contexte √©motionnel
- Informations date/heure en temps r√©el
- Informations m√©t√©o locales (Perpignan, Rivesaltes)
- Contr√¥le domotique
- Jeux (pendu, etc.)
- Aide g√©n√©rale avec contexte personnalis√©"""
        
        logging.debug(f"üå§Ô∏è [DEBUG] Weather info: {weather_info}")
        logging.debug(f"üß† [DEBUG] User context: {user_context_str}")
        logging.debug(f"üìù [DEBUG] System prompt length: {len(system_prompt)}")
        
        # Utiliser Ollama pour g√©n√©rer la r√©ponse avec context manager corrig√©
        try:
            logging.debug(f"ü§ñ [PROCESS] G√©n√©ration r√©ponse avec Ollama...")
            
            # V√©rifier que le client Ollama global est disponible avant utilisation
            if not check_service_initialized("ollama_client", ollama_client):
                logging.error("‚ùå [PROCESS] Client Ollama non initialis√©")
                return "Service IA temporairement indisponible, veuillez r√©essayer."
            
            # Utiliser le client global avec gestion d'erreur robuste
            try:
                response = await ollama_client.chat(
                    model="llama3.2:1b",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": message}
                    ],
                    temperature=0.7,
                    max_tokens=512
                )
                
                if response:
                    logging.info(f"‚úÖ [PROCESS] R√©ponse Ollama g√©n√©r√©e: {response[:50]}...")
                    return response.strip()
                else:
                    logging.warning(f"‚ö†Ô∏è [PROCESS] Ollama a retourn√© une r√©ponse vide")
                    return "D√©sol√©, je n'ai pas pu traiter votre demande."
                    
            except asyncio.TimeoutError:
                logging.error("‚ùå [PROCESS] Timeout Ollama - Service trop lent")
                return "Le service IA met trop de temps √† r√©pondre, veuillez r√©essayer."
            except ConnectionError as e:
                logging.error(f"‚ùå [PROCESS] Erreur connexion Ollama: {e}")
                return "Service IA temporairement indisponible, veuillez r√©essayer."
                
        except Exception as e:
            logging.error(f"‚ùå [PROCESS] Erreur g√©n√©ration Ollama: {e}")
            return "Une erreur s'est produite lors de la g√©n√©ration de la r√©ponse."
            
    except Exception as e:
        logging.error(f"‚ùå [PROCESS] Erreur traitement message: {e}")
        return "Une erreur s'est produite lors du traitement de votre message."

# =============================================================================
# ENDPOINTS M√âMOIRE NEUROMORPHIQUE
# =============================================================================

@app.get("/memory/{user_id}")
async def get_user_memory(user_id: str):
    """R√©cup√®re la m√©moire d'un utilisateur"""
    try:
        logging.info(f"üß† [MEMORY] R√©cup√©ration m√©moire pour: {user_id}")
        
        # Utiliser le syst√®me de m√©moire existant
        if hasattr(brain_memory_system, 'get_user_memories'):
            memories = await brain_memory_system.get_user_memories(user_id)
        else:
            # Fallback - r√©cup√©ration basique
            memories = {
                "user_id": user_id,
                "total_interactions": 0,
                "recent_memories": [],
                "status": "memory_system_available"
            }
        
        return {
            "user_id": user_id,
            "memories": memories,
            "timestamp": datetime.now().isoformat(),
            "status": "success"
        }
        
    except Exception as e:
        logging.error(f"‚ùå [MEMORY] Erreur r√©cup√©ration m√©moire: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur m√©moire: {str(e)}")

@app.post("/memory/{user_id}/store")
async def store_user_memory(user_id: str, memory_data: dict):
    """Stocke une nouvelle m√©moire pour un utilisateur"""
    try:
        logging.info(f"üß† [MEMORY] Stockage m√©moire pour: {user_id}")
        
        content = memory_data.get('content', '')
        memory_type = memory_data.get('type', 'episodic')
        importance = memory_data.get('importance', 5)
        
        # Utiliser le syst√®me de m√©moire existant
        if hasattr(brain_memory_system, 'store_interaction'):
            result = await brain_memory_system.store_interaction(
                user_id, content, f"M√©moire {memory_type}: {content}"
            )
        else:
            result = {"status": "stored", "memory_id": f"mem_{int(datetime.now().timestamp())}"}
        
        return {
            "user_id": user_id,
            "memory_stored": result,
            "timestamp": datetime.now().isoformat(),
            "status": "success"
        }
        
    except Exception as e:
        logging.error(f"‚ùå [MEMORY] Erreur stockage m√©moire: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur stockage: {str(e)}")

@app.get("/memory/{user_id}/search")
async def search_user_memory(user_id: str, query: str):
    """Recherche dans la m√©moire d'un utilisateur"""
    try:
        logging.info(f"üß† [MEMORY] Recherche m√©moire pour: {user_id}, query: {query}")
        
        # Utiliser le syst√®me de m√©moire existant
        if hasattr(brain_memory_system, 'search_memories'):
            results = await brain_memory_system.search_memories(user_id, query)
        else:
            # Fallback - recherche simul√©e
            results = {
                "query": query,
                "matches": [],
                "total_found": 0
            }
        
        return {
            "user_id": user_id,
            "query": query,
            "results": results,
            "timestamp": datetime.now().isoformat(),
            "status": "success"
        }
        
    except Exception as e:
        logging.error(f"‚ùå [MEMORY] Erreur recherche m√©moire: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur recherche: {str(e)}")

# =============================================================================
# ENDPOINTS OLLAMA 
# =============================================================================

@app.get("/ollama/models")
async def get_ollama_models():
    """Liste des mod√®les Ollama disponibles"""
    try:
        logging.info("ü§ñ [OLLAMA] R√©cup√©ration liste mod√®les")
        
        if ollama_client:
            models = await ollama_client.list_models()
            return {
                "models": models,
                "count": len(models) if models else 0,
                "timestamp": datetime.now().isoformat(),
                "status": "success"
            }
        else:
            return {
                "models": [],
                "count": 0,
                "error": "Ollama client non initialis√©",
                "status": "unavailable"
            }
            
    except Exception as e:
        logging.error(f"‚ùå [OLLAMA] Erreur r√©cup√©ration mod√®les: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur Ollama: {str(e)}")

@app.get("/ollama/status")
async def get_ollama_status():
    """Statut du service Ollama"""
    try:
        logging.info("ü§ñ [OLLAMA] V√©rification statut")
        
        if ollama_client:
            # Test de connexion simple
            models = await ollama_client.list_models()
            is_available = models is not None
            
            return {
                "available": is_available,
                "models_count": len(models) if models else 0,
                "client_initialized": True,
                "timestamp": datetime.now().isoformat(),
                "status": "success"
            }
        else:
            return {
                "available": False,
                "models_count": 0,
                "client_initialized": False,
                "error": "Client non initialis√©",
                "status": "unavailable"
            }
            
    except Exception as e:
        logging.error(f"‚ùå [OLLAMA] Erreur v√©rification statut: {e}")
        return {
            "available": False,
            "error": str(e),
            "status": "error"
        }

@app.post("/ollama/generate")
async def generate_ollama_response(request: dict):
    """G√©n√®re une r√©ponse avec Ollama"""
    try:
        prompt = request.get('prompt', '')
        model = request.get('model', 'llama3.2:1b')
        
        logging.info(f"ü§ñ [OLLAMA] G√©n√©ration avec mod√®le: {model}")
        
        if not ollama_client:
            raise HTTPException(status_code=503, detail="Service Ollama indisponible")
        
        response = await ollama_client.generate(
            model=model,
            prompt=prompt,
            options={
                "temperature": request.get('temperature', 0.7),
                "max_tokens": request.get('max_tokens', 512)
            }
        )
        
        return {
            "prompt": prompt,
            "response": response,
            "model": model,
            "timestamp": datetime.now().isoformat(),
            "status": "success"
        }
        
    except Exception as e:
        logging.error(f"‚ùå [OLLAMA] Erreur g√©n√©ration: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur g√©n√©ration: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)