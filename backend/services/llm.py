"""Service LLM avec Ollama - timeouts et retries"""
import asyncio
import logging
import httpx
import time
from typing import Optional, Dict, List, Any
from datetime import datetime
import random

# Import m√©triques Prometheus
try:
    from ..observability.metrics import llm_latency, update_service_health, service_response_time
except ImportError:
    # Fallback si prometheus_client pas install√©
    def llm_latency_observe(*args, **kwargs):
        pass
    llm_latency = type('MockHistogram', (), {'observe': llm_latency_observe})()
    def update_service_health(*args, **kwargs):
        pass
    service_response_time = type('MockHistogram', (), {'labels': lambda **kw: type('', (), {'observe': lambda x: None})()})()

logger = logging.getLogger(__name__)

class LLMService:
    """Service centralis√© pour interactions avec Ollama LLM"""
    
    def __init__(self, settings):
        self.settings = settings
        self.ollama_client = None
        self.model_name = "llama3.2:1b"
        # Client HTTP avec timeouts robustes
        self.http_client = httpx.AsyncClient(
            timeout=httpx.Timeout(10.0, connect=5.0),
            limits=httpx.Limits(max_connections=10, max_keepalive_connections=5)
        )
        
    async def initialize(self):
        """Initialise le client Ollama"""
        try:
            # Import dynamique pour √©viter d√©pendance au d√©marrage
            from integration.ollama_client import OllamaClient
            
            self.ollama_client = OllamaClient(base_url=self.settings.ollama_base_url)
            
            logger.info("ü§ñ [LLM] V√©rification disponibilit√© Ollama...")
            if await self.ollama_client.is_available():
                logger.info("ü§ñ [LLM] Service Ollama disponible")
                
                if hasattr(self.ollama_client, 'ensure_model_available'):
                    if await self.ollama_client.ensure_model_available(self.model_name):
                        logger.info(f"‚úÖ [LLM] Mod√®le {self.model_name} pr√™t")
                    else:
                        logger.warning(f"‚ö†Ô∏è [LLM] Mod√®le {self.model_name} non disponible")
                else:
                    logger.warning("‚ö†Ô∏è [LLM] Fonction ensure_model_available non disponible")
            else:
                logger.warning("‚ö†Ô∏è [LLM] Service Ollama non disponible")
                
        except Exception as e:
            logger.error(f"‚ùå [LLM] Erreur initialisation: {e}")
    
    async def close(self):
        """Ferme proprement les clients"""
        if self.ollama_client:
            try:
                if hasattr(self.ollama_client, 'client') and self.ollama_client.client:
                    await self.ollama_client.client.aclose()
                    logger.info("‚úÖ [LLM] Client Ollama ferm√©")
                else:
                    logger.info("‚ÑπÔ∏è [LLM] Client d√©j√† ferm√© ou non initialis√©")
            except Exception as e:
                logger.error(f"‚ùå [LLM] Erreur fermeture: {e}")
        
        # Fermer client HTTP
        await self.http_client.aclose()
        
    async def _retry_with_backoff(self, func, max_retries=3, base_delay=1.0):
        """Retry avec backoff exponentiel"""
        for attempt in range(max_retries):
            try:
                return await func()
            except (httpx.TimeoutException, httpx.ConnectError, asyncio.TimeoutError) as e:
                if attempt == max_retries - 1:
                    raise e
                
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                logger.warning(f"‚ö†Ô∏è [LLM] Tentative {attempt + 1}/{max_retries} √©chou√©e, retry dans {delay:.1f}s: {e}")
                await asyncio.sleep(delay)
    
    def is_available(self) -> bool:
        """V√©rifie si le service LLM est disponible"""
        return self.ollama_client is not None
    
    async def ping(self) -> bool:
        """Health check pour readiness probe avec m√©triques"""
        start_time = time.perf_counter()
        is_healthy = False
        
        try:
            async def _ping():
                url = f"{self.settings.ollama_base_url}/api/version"
                resp = await self.http_client.get(url)
                return resp.status_code == 200
            
            is_healthy = await self._retry_with_backoff(_ping, max_retries=2, base_delay=0.5)
            return is_healthy
        except Exception:
            return False
        finally:
            # Enregistrer m√©triques
            duration = time.perf_counter() - start_time
            service_response_time.labels(service="ollama").observe(duration)
            update_service_health("ollama", is_healthy)
    
    async def generate_response(
        self,
        message: str,
        context: Dict[str, Any],
        user_id: str = "default"
    ) -> str:
        """
        G√©n√®re une r√©ponse IA avec contexte et m√©triques
        Extrait de la fonction process_message() de main.py:530-696
        """
        start_time = time.perf_counter()
        
        try:
            if not self.is_available():
                logger.error("‚ùå [LLM] Client Ollama non initialis√©")
                return "Service IA temporairement indisponible, veuillez r√©essayer."
            
            # Construction du prompt syst√®me (extrait de main.py:611-648)
            system_prompt = self._build_system_prompt(context, user_id)
            
            logger.debug(f"ü§ñ [LLM] G√©n√©ration r√©ponse avec Ollama...")
            
            try:
                response = await self.ollama_client.chat(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": message}
                    ],
                    temperature=0.7,
                    max_tokens=512
                )
                
                if response:
                    logger.info(f"‚úÖ [LLM] R√©ponse g√©n√©r√©e: {response[:50]}...")
                    return response.strip()
                else:
                    logger.warning("‚ö†Ô∏è [LLM] Ollama a retourn√© une r√©ponse vide")
                    return "D√©sol√©, je n'ai pas pu traiter votre demande."
                    
            except asyncio.TimeoutError:
                logger.error("‚ùå [LLM] Timeout Ollama - Service trop lent")
                return "Le service IA met trop de temps √† r√©pondre, veuillez r√©essayer."
            except ConnectionError as e:
                logger.error(f"‚ùå [LLM] Erreur connexion Ollama: {e}")
                return "Service IA temporairement indisponible, veuillez r√©essayer."
                
        except Exception as e:
            logger.error(f"‚ùå [LLM] Erreur g√©n√©ration: {e}")
            return "Une erreur s'est produite lors de la g√©n√©ration de la r√©ponse."
        finally:
            # Enregistrer latence LLM dans m√©triques
            duration = time.perf_counter() - start_time
            llm_latency.observe(duration)
    
    def _build_system_prompt(
        self, 
        context: Dict[str, Any], 
        user_id: str
    ) -> str:
        """
        Construit le prompt syst√®me avec contexte
        Extrait de main.py:611-648 
        """
        current_time = datetime.now()
        
        # Noms fran√ßais des jours et mois (extrait main.py:587-591)
        french_days = ['lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche']
        french_months = ['janvier', 'f√©vrier', 'mars', 'avril', 'mai', 'juin', 
                        'juillet', 'ao√ªt', 'septembre', 'octobre', 'novembre', 'd√©cembre']
        
        french_date = f"{french_days[current_time.weekday()]} {current_time.day} {french_months[current_time.month-1]} {current_time.year} √† {current_time.strftime('%H:%M:%S')}"
        
        # Contexte m√©t√©o s'il existe
        weather_info = context.get('weather_info', '')
        
        # Contexte m√©moire utilisateur s'il existe  
        user_context_str = context.get('user_context_str', '')
        
        return f"""Tu es Jarvis, l'assistant IA personnel d'Enzo.

PROFIL UTILISATEUR :
- Nom : Enzo
- √Çge : 21 ans 
- Localisation : Perpignan, Pyr√©n√©es-Orientales (66), France
- Profil : Futur ing√©nieur r√©seau/cybers√©curit√©, passionn√© technologie

üß† SYST√àME M√âMOIRE NEUROMORPHIQUE ACTIF :
- Architecture inspir√©e du cerveau humain (limbique/pr√©frontal/hippocampe)
- Analyse √©motionnelle des interactions pour pond√©rer les souvenirs
- Consolidation automatique des m√©moires importantes
- Recherche vectorielle s√©mantique avec Qdrant

INFORMATIONS TEMPS R√âEL :
- Date et heure actuelles : {french_date}
- Localisation : Perpignan, Pyr√©n√©es-Orientales, France

{weather_info}{user_context_str}

R√àGLES ABSOLUES :
- TOUJOURS r√©pondre en fran√ßais parfait et naturel
- Tu es JARVIS, l'assistant IA. Enzo est ton utilisateur (21 ans, Perpignan)
- NE JAMAIS dire que TU as un √¢ge - tu es une IA
- Utiliser OBLIGATOIREMENT les informations temps r√©el et m√©t√©o ci-dessus
- Utiliser les m√©moires contextuelles neuromorphiques pour personnaliser les r√©ponses
- Si des donn√©es m√©t√©o sont fournies, les citer EXACTEMENT dans ta r√©ponse
- Ne JAMAIS dire que tu n'as pas acc√®s aux informations si elles sont fournies
- √ätre concis, pr√©cis et amical avec Enzo
- Utiliser les donn√©es de la m√©moire neuromorphique, jamais d'invention

CAPACIT√âS AVANC√âES :
- M√©moire neuromorphique avec contexte √©motionnel
- Informations date/heure en temps r√©el
- Informations m√©t√©o locales (Perpignan, Rivesaltes)
- Contr√¥le domotique
- Jeux (pendu, etc.)
- Aide g√©n√©rale avec contexte personnalis√©"""