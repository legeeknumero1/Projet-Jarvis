"""Service LLM avec Ollama - extrait de main.py"""
import asyncio
import logging
from typing import Optional, Dict, List, Any
from datetime import datetime

logger = logging.getLogger(__name__)

class LLMService:
    """Service centralis√© pour interactions avec Ollama LLM"""
    
    def __init__(self, settings):
        self.settings = settings
        self.ollama_client = None
        self.model_name = "llama3.2:1b"
        
    async def initialize(self):
        """Initialise le client Ollama"""
        try:
            # Import dynamique pour √©viter d√©pendance au d√©marrage
            from integration.ollama_client import OllamaClient
            
            self.ollama_client = OllamaClient(base_url=self.settings.ollama_base_url)
            
            logger.info("ü§ñ [LLM] V√©rification disponibilit√© Ollama...")
            if await self.ollama_client.is_available():
                logger.info("ü§ñ [LLM] Service Ollama disponible")
                
                if hasattr(self.ollama_client, 'ensure_model_available'):
                    if await self.ollama_client.ensure_model_available(self.model_name):
                        logger.info(f"‚úÖ [LLM] Mod√®le {self.model_name} pr√™t")
                    else:
                        logger.warning(f"‚ö†Ô∏è [LLM] Mod√®le {self.model_name} non disponible")
                else:
                    logger.warning("‚ö†Ô∏è [LLM] Fonction ensure_model_available non disponible")
            else:
                logger.warning("‚ö†Ô∏è [LLM] Service Ollama non disponible")
                
        except Exception as e:
            logger.error(f"‚ùå [LLM] Erreur initialisation: {e}")
    
    async def close(self):
        """Ferme proprement le client Ollama"""
        if self.ollama_client:
            try:
                if hasattr(self.ollama_client, 'client') and self.ollama_client.client:
                    await self.ollama_client.client.aclose()
                    logger.info("‚úÖ [LLM] Client Ollama ferm√©")
                else:
                    logger.info("‚ÑπÔ∏è [LLM] Client d√©j√† ferm√© ou non initialis√©")
            except Exception as e:
                logger.error(f"‚ùå [LLM] Erreur fermeture: {e}")
    
    def is_available(self) -> bool:
        """V√©rifie si le service LLM est disponible"""
        return self.ollama_client is not None
    
    async def generate_response(
        self,
        message: str,
        context: Dict[str, Any],
        user_id: str = "default"
    ) -> str:
        """
        G√©n√®re une r√©ponse IA avec contexte
        Extrait de la fonction process_message() de main.py:530-696
        """
        try:
            if not self.is_available():
                logger.error("‚ùå [LLM] Client Ollama non initialis√©")
                return "Service IA temporairement indisponible, veuillez r√©essayer."
            
            # Construction du prompt syst√®me (extrait de main.py:611-648)
            system_prompt = self._build_system_prompt(context, user_id)
            
            logger.debug(f"ü§ñ [LLM] G√©n√©ration r√©ponse avec Ollama...")
            
            try:
                response = await self.ollama_client.chat(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": message}
                    ],
                    temperature=0.7,
                    max_tokens=512
                )
                
                if response:
                    logger.info(f"‚úÖ [LLM] R√©ponse g√©n√©r√©e: {response[:50]}...")
                    return response.strip()
                else:
                    logger.warning("‚ö†Ô∏è [LLM] Ollama a retourn√© une r√©ponse vide")
                    return "D√©sol√©, je n'ai pas pu traiter votre demande."
                    
            except asyncio.TimeoutError:
                logger.error("‚ùå [LLM] Timeout Ollama - Service trop lent")
                return "Le service IA met trop de temps √† r√©pondre, veuillez r√©essayer."
            except ConnectionError as e:
                logger.error(f"‚ùå [LLM] Erreur connexion Ollama: {e}")
                return "Service IA temporairement indisponible, veuillez r√©essayer."
                
        except Exception as e:
            logger.error(f"‚ùå [LLM] Erreur g√©n√©ration: {e}")
            return "Une erreur s'est produite lors de la g√©n√©ration de la r√©ponse."
    
    def _build_system_prompt(
        self, 
        context: Dict[str, Any], 
        user_id: str
    ) -> str:
        """
        Construit le prompt syst√®me avec contexte
        Extrait de main.py:611-648 
        """
        current_time = datetime.now()
        
        # Noms fran√ßais des jours et mois (extrait main.py:587-591)
        french_days = ['lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche']
        french_months = ['janvier', 'f√©vrier', 'mars', 'avril', 'mai', 'juin', 
                        'juillet', 'ao√ªt', 'septembre', 'octobre', 'novembre', 'd√©cembre']
        
        french_date = f"{french_days[current_time.weekday()]} {current_time.day} {french_months[current_time.month-1]} {current_time.year} √† {current_time.strftime('%H:%M:%S')}"
        
        # Contexte m√©t√©o s'il existe
        weather_info = context.get('weather_info', '')
        
        # Contexte m√©moire utilisateur s'il existe  
        user_context_str = context.get('user_context_str', '')
        
        return f"""Tu es Jarvis, l'assistant IA personnel d'Enzo.

PROFIL UTILISATEUR :
- Nom : Enzo
- √Çge : 21 ans 
- Localisation : Perpignan, Pyr√©n√©es-Orientales (66), France
- Profil : Futur ing√©nieur r√©seau/cybers√©curit√©, passionn√© technologie

üß† SYST√àME M√âMOIRE NEUROMORPHIQUE ACTIF :
- Architecture inspir√©e du cerveau humain (limbique/pr√©frontal/hippocampe)
- Analyse √©motionnelle des interactions pour pond√©rer les souvenirs
- Consolidation automatique des m√©moires importantes
- Recherche vectorielle s√©mantique avec Qdrant

INFORMATIONS TEMPS R√âEL :
- Date et heure actuelles : {french_date}
- Localisation : Perpignan, Pyr√©n√©es-Orientales, France

{weather_info}{user_context_str}

R√àGLES ABSOLUES :
- TOUJOURS r√©pondre en fran√ßais parfait et naturel
- Tu es JARVIS, l'assistant IA. Enzo est ton utilisateur (21 ans, Perpignan)
- NE JAMAIS dire que TU as un √¢ge - tu es une IA
- Utiliser OBLIGATOIREMENT les informations temps r√©el et m√©t√©o ci-dessus
- Utiliser les m√©moires contextuelles neuromorphiques pour personnaliser les r√©ponses
- Si des donn√©es m√©t√©o sont fournies, les citer EXACTEMENT dans ta r√©ponse
- Ne JAMAIS dire que tu n'as pas acc√®s aux informations si elles sont fournies
- √ätre concis, pr√©cis et amical avec Enzo
- Utiliser les donn√©es de la m√©moire neuromorphique, jamais d'invention

CAPACIT√âS AVANC√âES :
- M√©moire neuromorphique avec contexte √©motionnel
- Informations date/heure en temps r√©el
- Informations m√©t√©o locales (Perpignan, Rivesaltes)
- Contr√¥le domotique
- Jeux (pendu, etc.)
- Aide g√©n√©rale avec contexte personnalis√©"""