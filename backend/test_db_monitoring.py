#!/usr/bin/env python3
"""
Script de test pour le systÃ¨me de monitoring des requÃªtes PostgreSQL
Test de performance et validation des mÃ©triques
"""

import asyncio
import asyncpg
import json
import time
import logging
from datetime import datetime
from typing import List, Dict, Any
import sys
import os

# Ajouter le backend au PYTHONPATH
sys.path.insert(0, '/home/enzo/Projet-Jarvis/backend')

from monitoring.query_monitor import db_monitor, QueryMetrics
from config.config import Config

# Configuration logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_database_monitoring():
    """Test complet du systÃ¨me de monitoring"""
    
    print("ğŸ” [TEST] DÃ©marrage des tests de monitoring PostgreSQL...")
    
    # 1. Initialiser le systÃ¨me de monitoring
    config = Config()
    database_url = config.database_url
    
    try:
        await db_monitor.initialize_analysis_pool(database_url)
        print("âœ… [TEST] Pool d'analyse initialisÃ©")
    except Exception as e:
        print(f"âŒ [TEST] Erreur initialisation pool: {e}")
        return False
    
    # 2. Test d'enregistrement de requÃªtes
    print("\nğŸ“Š [TEST] Test d'enregistrement des mÃ©triques...")
    
    test_queries = [
        ("SELECT * FROM users WHERE id = $1", 0.5, "SELECT", "users"),
        ("SELECT * FROM conversations WHERE user_id = $1 ORDER BY timestamp DESC LIMIT 10", 2.1, "SELECT", "conversations"),
        ("INSERT INTO memories (user_id, content) VALUES ($1, $2)", 0.3, "INSERT", "memories"),
        ("UPDATE users SET preferences = $1 WHERE id = $2", 1.8, "UPDATE", "users"),
        ("SELECT COUNT(*) FROM conversations", 3.2, "SELECT", "conversations"),
    ]
    
    for query, duration, operation, table in test_queries:
        await db_monitor.record_query(
            query=query,
            duration=duration,
            operation=operation,
            table=table,
            user_id="test_user"
        )
        print(f"  ğŸ“ RequÃªte enregistrÃ©e: {operation} on {table} ({duration}s)")
    
    # 3. VÃ©rifier les statistiques
    print("\nğŸ“ˆ [TEST] VÃ©rification des statistiques...")
    
    stats = db_monitor.get_query_statistics()
    
    print(f"  ğŸ”¢ Total des requÃªtes: {stats['summary']['total_queries']}")
    print(f"  âš ï¸ RequÃªtes lentes: {stats['summary']['total_slow_queries']}")
    print(f"  ğŸ“Š Taux de requÃªtes lentes: {stats['summary']['slow_query_rate']:.1f}%")
    print(f"  ğŸ·ï¸ Types de requÃªtes monitorÃ©s: {stats['summary']['monitored_query_types']}")
    
    # Afficher dÃ©tails par type
    print("\nğŸ“‹ [TEST] DÃ©tails par type de requÃªte:")
    for query_type, query_stats in stats['query_stats'].items():
        print(f"  ğŸ” {query_type}:")
        print(f"    - Total: {query_stats['total_count']}")
        print(f"    - Lentes: {query_stats['slow_count']}")
        print(f"    - DurÃ©e moyenne: {query_stats['avg_duration']:.2f}s")
        print(f"    - P95: {query_stats['p95_duration']:.2f}s")
        print(f"    - Taux lent: {query_stats['slow_rate']:.1f}%")
    
    # 4. Test des mÃ©triques de base de donnÃ©es
    print("\nğŸ—„ï¸ [TEST] Test des mÃ©triques de base de donnÃ©es...")
    
    try:
        db_metrics = await db_monitor.get_database_metrics()
        print(f"  ğŸ”— Connexions actives: {db_metrics.get('active_connections', 'N/A')}")
        print(f"  ğŸ’¾ Taille DB: {db_metrics.get('database_size', 'N/A')}")
        print(f"  ğŸ“Š Ratio cache hit: {db_metrics.get('cache_hit_ratio', 'N/A')}%")
        
        # Afficher activitÃ© des tables
        table_activity = db_metrics.get('table_activity', [])
        if table_activity:
            print("  ğŸ“Š Top 5 tables par activitÃ©:")
            for table in table_activity[:5]:
                print(f"    - {table.get('tablename', 'N/A')}: {table.get('total_activity', 0)} operations")
    except Exception as e:
        print(f"  âš ï¸ MÃ©triques DB non disponibles: {e}")
    
    # 5. Test de gÃ©nÃ©ration de requÃªtes lentes artificielles
    print("\nğŸŒ [TEST] GÃ©nÃ©ration de requÃªtes lentes pour test...")
    
    slow_queries = [
        ("SELECT pg_sleep(2); SELECT COUNT(*) FROM pg_stat_activity", 2.5, "SELECT", "pg_stat_activity"),
        ("SELECT * FROM conversations ORDER BY timestamp DESC", 4.2, "SELECT", "conversations"),
        ("UPDATE users SET last_accessed = NOW() WHERE created_at < NOW() - INTERVAL '1 day'", 6.1, "UPDATE", "users")
    ]
    
    for query, duration, operation, table in slow_queries:
        await db_monitor.record_query(
            query=query,
            duration=duration,
            operation=operation,
            table=table,
            user_id="test_slow"
        )
        print(f"  ğŸŒ RequÃªte lente simulÃ©e: {operation} on {table} ({duration}s)")
    
    # 6. VÃ©rifier les requÃªtes lentes rÃ©centes
    print("\nğŸ” [TEST] RequÃªtes lentes rÃ©centes:")
    
    updated_stats = db_monitor.get_query_statistics()
    recent_slow = updated_stats.get('recent_slow_queries', [])
    
    for slow_query in recent_slow[-3:]:  # 3 plus rÃ©centes
        print(f"  âš ï¸ {slow_query['operation']} on {slow_query['table']}")
        print(f"     DurÃ©e: {slow_query['duration']}s")
        print(f"     Timestamp: {slow_query['timestamp']}")
        print(f"     Query: {slow_query['query'][:80]}...")
    
    # 7. Test de nettoyage
    print("\nğŸ§¹ [TEST] Test de nettoyage...")
    
    try:
        await db_monitor.cleanup()
        print("âœ… [TEST] Nettoyage rÃ©ussi")
    except Exception as e:
        print(f"âŒ [TEST] Erreur nettoyage: {e}")
    
    print("\nâœ… [TEST] Tests de monitoring terminÃ©s avec succÃ¨s!")
    return True

async def test_performance_monitoring():
    """Test de performance du systÃ¨me de monitoring"""
    
    print("\nğŸš€ [PERF] Test de performance du monitoring...")
    
    # GÃ©nÃ©rer 100 requÃªtes rapidement
    start_time = time.time()
    
    for i in range(100):
        await db_monitor.record_query(
            query=f"SELECT * FROM test_table_{i % 5} WHERE id = $1",
            duration=0.1 + (i % 10) * 0.05,  # DurÃ©es variables
            operation="SELECT",
            table=f"test_table_{i % 5}",
            user_id=f"perf_user_{i % 3}"
        )
    
    processing_time = time.time() - start_time
    print(f"ğŸ“Š [PERF] 100 requÃªtes traitÃ©es en {processing_time:.2f}s")
    print(f"ğŸ“Š [PERF] DÃ©bit: {100/processing_time:.0f} requÃªtes/seconde")
    
    # VÃ©rifier les stats finales
    final_stats = db_monitor.get_query_statistics()
    print(f"ğŸ“Š [PERF] Total requÃªtes monitorÃ©es: {final_stats['summary']['total_queries']}")
    print(f"ğŸ“Š [PERF] Types uniques: {final_stats['summary']['monitored_query_types']}")

async def test_api_endpoints():
    """Test des endpoints API de monitoring"""
    
    print("\nğŸŒ [API] Test des endpoints de monitoring...")
    
    # Simuler des appels aux endpoints
    try:
        import httpx
        import asyncio
        
        async with httpx.AsyncClient() as client:
            base_url = "http://localhost:8000"
            
            # Test health endpoint
            try:
                response = await client.get(f"{base_url}/monitoring/health")
                if response.status_code == 200:
                    print("âœ… [API] /monitoring/health - OK")
                    health_data = response.json()
                    print(f"  Status: {health_data.get('status')}")
                else:
                    print(f"âš ï¸ [API] /monitoring/health - Status: {response.status_code}")
            except Exception as e:
                print(f"âŒ [API] /monitoring/health non disponible: {e}")
            
            # Test metrics endpoint  
            try:
                response = await client.get(f"{base_url}/monitoring/metrics")
                if response.status_code == 200:
                    print("âœ… [API] /monitoring/metrics - OK")
                    print(f"  Content-Type: {response.headers.get('content-type')}")
                else:
                    print(f"âš ï¸ [API] /monitoring/metrics - Status: {response.status_code}")
            except Exception as e:
                print(f"âŒ [API] /monitoring/metrics non disponible: {e}")
                
    except ImportError:
        print("âš ï¸ [API] httpx non disponible - tests API ignorÃ©s")

async def main():
    """Point d'entrÃ©e principal des tests"""
    
    print("ğŸ§ª Jarvis - Tests du systÃ¨me de monitoring PostgreSQL")
    print("=" * 60)
    
    try:
        # Test principal
        success = await test_database_monitoring()
        
        if success:
            # Tests de performance
            await test_performance_monitoring()
            
            # Tests API (optionnels)
            await test_api_endpoints()
            
            print("\n" + "=" * 60)
            print("ğŸ‰ [SUCCESS] Tous les tests de monitoring rÃ©ussis!")
            print("ğŸ“Š Le systÃ¨me de monitoring PostgreSQL est opÃ©rationnel")
            
            # Afficher rÃ©sumÃ© final
            final_stats = db_monitor.get_query_statistics()
            print(f"\nğŸ“ˆ RÃ©sumÃ© final:")
            print(f"  - Total requÃªtes monitorÃ©es: {final_stats['summary']['total_queries']}")
            print(f"  - RequÃªtes lentes dÃ©tectÃ©es: {final_stats['summary']['total_slow_queries']}")
            print(f"  - Taux de requÃªtes lentes: {final_stats['summary']['slow_query_rate']:.1f}%")
            
        else:
            print("\nâŒ [FAILURE] Tests Ã©chouÃ©s")
            return 1
            
    except Exception as e:
        print(f"\nğŸ’¥ [ERROR] Erreur critique: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    # ExÃ©cuter les tests
    exit_code = asyncio.run(main())
    sys.exit(exit_code)