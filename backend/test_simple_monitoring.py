#!/usr/bin/env python3
"""
Test simple du systÃ¨me de monitoring des requÃªtes PostgreSQL
"""

import asyncio
import time
import logging
import sys
import os

# Configuration logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_monitoring_core():
    """Test des fonctionnalitÃ©s core du monitoring"""
    
    print("ğŸ” [TEST] Test simple du monitoring PostgreSQL...")
    
    # Test direct des classes de monitoring
    from monitoring.query_monitor import DatabaseMonitor, QueryMetrics
    from datetime import datetime
    
    # Initialiser un monitor de test
    monitor = DatabaseMonitor(slow_query_threshold=1.0, enable_explain=False)
    
    print("âœ… [TEST] DatabaseMonitor initialisÃ©")
    
    # Test d'enregistrement de requÃªtes
    test_queries = [
        ("SELECT * FROM users WHERE id = $1", 0.3, "SELECT", "users"),
        ("SELECT * FROM conversations WHERE user_id = $1", 2.1, "SELECT", "conversations"),  # Lente
        ("INSERT INTO memories (user_id, content) VALUES ($1, $2)", 0.2, "INSERT", "memories"),
        ("UPDATE users SET preferences = $1 WHERE id = $2", 1.5, "UPDATE", "users"),  # Lente
        ("SELECT COUNT(*) FROM conversations", 0.4, "SELECT", "conversations"),
    ]
    
    for query, duration, operation, table in test_queries:
        await monitor.record_query(
            query=query,
            duration=duration,
            operation=operation,
            table=table,
            user_id="test_user"
        )
        print(f"  ğŸ“ RequÃªte enregistrÃ©e: {operation} on {table} ({duration}s)")
    
    # VÃ©rifier les statistiques
    stats = monitor.get_query_statistics()
    
    print(f"\nğŸ“Š [STATS] RÃ©sultats du monitoring:")
    print(f"  ğŸ”¢ Total des requÃªtes: {stats['summary']['total_queries']}")
    print(f"  âš ï¸ RequÃªtes lentes: {stats['summary']['total_slow_queries']}")
    print(f"  ğŸ“Š Taux de requÃªtes lentes: {stats['summary']['slow_query_rate']:.1f}%")
    print(f"  ğŸ·ï¸ Types de requÃªtes monitorÃ©s: {stats['summary']['monitored_query_types']}")
    
    # DÃ©tails par type
    print(f"\nğŸ“‹ [DETAILS] Par type de requÃªte:")
    for query_type, query_stats in stats['query_stats'].items():
        print(f"  ğŸ” {query_type}:")
        print(f"    - Total: {query_stats['total_count']}")
        print(f"    - Lentes: {query_stats['slow_count']}")
        print(f"    - DurÃ©e moyenne: {query_stats['avg_duration']:.2f}s")
        print(f"    - P95: {query_stats['p95_duration']:.2f}s")
        print(f"    - Taux lent: {query_stats['slow_rate']:.1f}%")
    
    # RequÃªtes lentes rÃ©centes
    print(f"\nğŸŒ [SLOW] RequÃªtes lentes rÃ©centes:")
    recent_slow = stats.get('recent_slow_queries', [])
    for slow_query in recent_slow:
        print(f"  âš ï¸ {slow_query['operation']} on {slow_query['table']} - {slow_query['duration']}s")
        print(f"     Query: {slow_query['query'][:60]}...")
    
    # Test de performance
    print(f"\nğŸš€ [PERF] Test de performance...")
    start_time = time.time()
    
    for i in range(50):
        await monitor.record_query(
            query=f"SELECT * FROM test_table_{i % 3} WHERE id = $1",
            duration=0.1 + (i % 5) * 0.02,
            operation="SELECT",
            table=f"test_table_{i % 3}",
            user_id=f"perf_user_{i % 2}"
        )
    
    processing_time = time.time() - start_time
    print(f"  ğŸ“Š 50 requÃªtes traitÃ©es en {processing_time:.3f}s")
    print(f"  ğŸ“Š DÃ©bit: {50/processing_time:.0f} requÃªtes/seconde")
    
    # Stats finales
    final_stats = monitor.get_query_statistics()
    print(f"\nâœ… [FINAL] Statistiques finales:")
    print(f"  - Total requÃªtes: {final_stats['summary']['total_queries']}")
    print(f"  - Types uniques: {final_stats['summary']['monitored_query_types']}")
    print(f"  - RequÃªtes lentes: {final_stats['summary']['total_slow_queries']}")
    
    return True

async def test_query_analyzer():
    """Test de l'analyseur de requÃªtes"""
    
    print(f"\nğŸ”¬ [ANALYZER] Test de l'analyseur de requÃªtes...")
    
    from monitoring.query_monitor import QueryAnalyzer
    
    test_cases = [
        ("SELECT * FROM users WHERE id = 1", "SELECT", "users"),
        ("INSERT INTO conversations (user_id, message) VALUES ($1, $2)", "INSERT", "conversations"),
        ("UPDATE memories SET content = $1 WHERE id = $2", "UPDATE", "memories"),
        ("DELETE FROM old_data WHERE created_at < NOW() - INTERVAL '1 year'", "DELETE", "old_data"),
        ("CREATE INDEX idx_user_timestamp ON conversations(user_id, timestamp)", "CREATE", "unknown"),
    ]
    
    for query, expected_op, expected_table in test_cases:
        parsed = QueryAnalyzer.parse_query(query)
        print(f"  ğŸ“ Query: {query[:50]}...")
        print(f"     Parsed: {parsed['operation']} on {parsed['table']}")
        
        # VÃ©rifier problÃ¨mes
        issues = QueryAnalyzer.is_query_problematic(query, 2.5)
        if issues:
            print(f"     Issues: {', '.join(issues)}")
    
    return True

async def main():
    """Point d'entrÃ©e principal"""
    
    print("ğŸ§ª Test simple du systÃ¨me de monitoring PostgreSQL")
    print("=" * 60)
    
    try:
        # Tests principaux
        success1 = await test_monitoring_core()
        success2 = await test_query_analyzer()
        
        if success1 and success2:
            print(f"\n" + "=" * 60)
            print("ğŸ‰ [SUCCESS] Tests simples rÃ©ussis!")
            print("ğŸ“Š Le systÃ¨me de monitoring fonctionne correctement")
        else:
            print(f"\nâŒ [FAILURE] Tests Ã©chouÃ©s")
            return 1
            
    except Exception as e:
        print(f"\nğŸ’¥ [ERROR] Erreur: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    # Ajouter le backend au PYTHONPATH
    sys.path.insert(0, '/home/enzo/Projet-Jarvis/backend')
    
    # ExÃ©cuter les tests
    exit_code = asyncio.run(main())
    sys.exit(exit_code)