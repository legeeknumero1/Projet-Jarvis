# ğŸ Jarvis Python Bridges IA - Phase 3

**Microservices Python pour services IA : Ollama (LLM), Whisper (STT), Piper (TTS), Embeddings**

Remplace les appels directs par une API HTTP centralisÃ©e et dÃ©couplÃ©e.

---

## âš¡ Architecture Phase 3

### Services ExposÃ©s

```
Python Bridges API (Port 8005)
â”œâ”€â”€ /api/llm/*          â†’ Ollama LLM client
â”œâ”€â”€ /api/stt/*          â†’ Whisper STT client
â”œâ”€â”€ /api/tts/*          â†’ Piper TTS client
â””â”€â”€ /api/embeddings/*   â†’ Sentence Transformers
```

### Communication

```
Rust Backend (8100)
    â†“
Python Bridges (8005) â† HTTP REST JSON
    â†“
â”œâ”€â”€ Ollama (11434)      â† LLM local
â”œâ”€â”€ Whisper (memory)    â† STT in-memory
â”œâ”€â”€ Piper (memory)      â† TTS in-memory
â””â”€â”€ S-BERT (memory)     â† Embeddings in-memory
```

---

## ğŸ¯ Endpoints API

### Health & Status

```bash
GET /health
# {
#   "status": "healthy",
#   "service": "python-bridges",
#   "version": "1.3.0",
#   "services": {
#     "ollama_llm": "âœ…",
#     "whisper_stt": "âœ…",
#     "piper_tts": "âœ…",
#     "embeddings": "âœ…"
#   }
# }

GET /ready
# {"status": "ready", "version": "1.3.0"}
```

### LLM - Ollama

```bash
POST /api/llm/generate
Content-Type: application/json

{
  "prompt": "Explique Python en franÃ§ais",
  "system_prompt": "Tu es un expert Python franÃ§ais",
  "temperature": 0.7,
  "max_tokens": 512
}

# Response
{
  "text": "Python est un langage...",
  "model": "llama2:7b",
  "tokens_generated": 128,
  "tokens_prompt": 15,
  "duration_ms": 2500
}
```

```bash
GET /api/llm/models
# {"models": ["llama2:7b", "neural-chat:7b", ...]}
```

### STT - Whisper

```bash
POST /api/stt/transcribe
Content-Type: application/json

{
  "audio_data": "base64_encoded_pcm_float32",
  "language": "fr"
}

# Response
{
  "text": "Bonjour, ceci est un test",
  "language": "fr",
  "confidence": 0.95,
  "duration_ms": 1200,
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 2.5,
      "text": "Bonjour, ceci est un test",
      "confidence": 0.95
    }
  ]
}
```

### TTS - Piper

```bash
POST /api/tts/synthesize
Content-Type: application/json

{
  "text": "Bonjour, comment allez-vous ?",
  "voice": "fr_FR-upmc-medium",
  "speed": 1.0
}

# Response
{
  "audio_data": "base64_encoded_float32",
  "sample_rate": 22050,
  "duration_ms": 2800,
  "voice": "fr_FR-upmc-medium"
}
```

```bash
GET /api/tts/voices
# {
#   "voices": {
#     "fr_FR-upmc-medium": "FranÃ§ais UPMC (femme) - RecommandÃ©",
#     "fr_FR-siwis-medium": "FranÃ§ais Siwis (femme)",
#     "fr_FR-tom-medium": "FranÃ§ais Tom (homme)"
#   }
# }
```

### Embeddings - Sentence Transformers

```bash
POST /api/embeddings/embed
Content-Type: application/json

{
  "text": "Ceci est un texte Ã  vectoriser"
}

# Response
{
  "text": "Ceci est un texte Ã  vectoriser",
  "vector": "base64_encoded_float32_vector",
  "dimension": 384
}
```

```bash
POST /api/embeddings/embed-batch
Content-Type: application/json

{
  "texts": [
    "Premier texte",
    "DeuxiÃ¨me texte",
    "TroisiÃ¨me texte"
  ]
}

# Response
{
  "embeddings": [
    {"text": "...", "vector": "...", "dimension": 384},
    ...
  ],
  "count": 3
}
```

---

## ğŸ³ Docker Integration

### DÃ©marrer le service

```bash
cd backend-python-bridges
docker-compose up -d

# VÃ©rifier santÃ©
curl http://localhost:8005/health
```

### Configuration

Le service s'intÃ¨gre dans l'infrastructure existante :
- Port 8005 pour API HTTP
- MÃªme rÃ©seau `jarvis_network` que Rust backend
- DÃ©pendances Ollama uniquement (autres services en mÃ©moire)
- Limite mÃ©moire 4GB (Whisper + embeddings)

---

## ğŸ“Š CaractÃ©ristiques Principales

### âœ… ImplÃ©mentÃ©

- **Ollama LLM Client** : HTTP interface vers LLM local
- **Whisper STT** : Speech-to-Text en-mÃ©moire (modÃ¨les tiny-large)
- **Piper TTS** : Text-to-Speech franÃ§ais haute qualitÃ©
- **Embeddings** : Vectorisation Sentence Transformers multilingue
- **API Flask** : REST HTTP pour tous les services
- **Health Checks** : Monitoring dÃ©taillÃ©
- **CORS** : AccÃ¨s cross-origin configurÃ©
- **Logging** : Logs structurÃ©s avec rotation

### ğŸ”„ En DÃ©veloppement

- **Streaming Ollama** : RÃ©ponses streaming token-par-token
- **Caching Embeddings** : Cache vecteurs pour perf
- **Model Management** : Changer modÃ¨les Ã  runtime
- **Rate Limiting** : ContrÃ´le dÃ©bit API

### ğŸ“‹ Roadmap

- **v1.3.2** : Streaming Ollama complÃ¨te
- **v1.4.0** : Caching distributed avec Redis
- **v1.5.0** : GPU acceleration (CUDA)

---

## ğŸ”Œ Communication avec Rust Backend

Le backend Rust appelle les services Python via HTTP :

```rust
// Dans le backend Rust
let client = reqwest::Client::new();

// LLM generation
let response = client
    .post("http://jarvis_python_bridges:8005/api/llm/generate")
    .json(&json!({"prompt": "..."}))
    .send()
    .await?;

// STT transcription
let response = client
    .post("http://jarvis_python_bridges:8005/api/stt/transcribe")
    .json(&json!({"audio_data": "..."}))
    .send()
    .await?;

// TTS synthesis
let response = client
    .post("http://jarvis_python_bridges:8005/api/tts/synthesize")
    .json(&json!({"text": "..."}))
    .send()
    .await?;

// Text embeddings
let response = client
    .post("http://jarvis_python_bridges:8005/api/embeddings/embed")
    .json(&json!({"text": "..."}))
    .send()
    .await?;
```

---

## ğŸ“ Fichiers du Projet

```
backend-python-bridges/
â”œâ”€â”€ app.py                 # ğŸš€ Application Flask principale
â”œâ”€â”€ ollama_client.py       # ğŸ¤– Client Ollama LLM
â”œâ”€â”€ whisper_client.py      # ğŸ¤ Client Whisper STT
â”œâ”€â”€ piper_client.py        # ğŸ”Š Client Piper TTS
â”œâ”€â”€ embeddings_service.py  # ğŸ§  Service Embeddings
â”œâ”€â”€ requirements.txt       # ğŸ“¦ DÃ©pendances Python
â”œâ”€â”€ Dockerfile             # ğŸ³ Build Docker
â”œâ”€â”€ docker-compose.yml     # ğŸ³ Service integration
â”œâ”€â”€ README.md              # ğŸ“– Ce fichier
â””â”€â”€ logs/                  # ğŸ“ Logs applicatif
```

---

## ğŸš€ Performance

### Benchmarks

```
STT (Whisper base):      ~5-10s pour 30s audio
TTS (Piper):             ~2-3s pour phrase
LLM (Ollama llama2:7b):  ~2-3 tokens/s
Embeddings:              ~0.2s pour 10 textes
```

### Optimisations

- ModÃ¨les chargÃ©s en mÃ©moire (pas de latence I/O)
- Batch processing pour embeddings
- Connection pooling pour Ollama
- Lazy loading des modÃ¨les

---

## ğŸ”§ Installation Locale

### PrÃ©requis

- Python 3.11+
- FFmpeg pour Whisper/Piper
- Service Ollama accessible (http://localhost:11434)

### Setup

```bash
# Clone et environnement
cd backend-python-bridges
python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate sur Windows

# Installer dÃ©pendances
pip install -r requirements.txt

# Lancer API
python app.py
# ğŸš€ Accessible sur http://localhost:8005
```

---

## ğŸ“ Fichiers du Projet

```
backend-python-bridges/
â”œâ”€â”€ app.py                 # Application Flask
â”œâ”€â”€ ollama_client.py       # Client LLM
â”œâ”€â”€ whisper_client.py      # Client STT
â”œâ”€â”€ piper_client.py        # Client TTS
â”œâ”€â”€ embeddings_service.py  # Service embeddings
â”œâ”€â”€ requirements.txt       # DÃ©pendances
â”œâ”€â”€ Dockerfile             # Build Docker
â”œâ”€â”€ docker-compose.yml     # Orchestration
â””â”€â”€ README.md              # Cette doc
```

---

## ğŸ¤ IntÃ©gration Architecture Polyglotte

**Phase 3 dans le contexte global :**

- ğŸ¦€ **Phase 1 (DONE)** : Rust API Core
- âš™ï¸ **Phase 2 (DONE)** : C++ Audio Engine
- ğŸ **Phase 3 (YOU ARE HERE)** : Python IA Bridges
- ğŸ¦€ **Phase 4** : Rust DB Layer
- ğŸ¹ **Phase 5** : Go Monitoring
- ğŸŒ **Phase 6+** : Frontend + Plugins

**Phase 3 apporte :**
- âœ… DÃ©couplage services IA via HTTP
- âœ… FacilitÃ© de remplacement (swap modÃ¨les)
- âœ… Scaling indÃ©pendant
- âœ… Monitoring centralisÃ©

---

**ğŸ Python Bridges IA - Services dÃ©couplÃ©s haute performance**

*Architecture Polyglotte Phase 3 pour Jarvis AI Assistant*
